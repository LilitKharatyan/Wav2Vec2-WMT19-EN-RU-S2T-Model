!python --version
!nvidia-smi
!pip install -q transformers
!pip install -q pydub
!pip install torch 
!pip install sacremoses
import os
import time
import psutil
import nltk
import librosa
import torch
import soundfile as sf
from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer, FSMTForConditionalGeneration, FSMTTokenizer
from google.colab import drive

nltk.download('punkt')
drive.mount('/content/drive')
def load_wav2vec_960h_model():
    """
    Returns the tokenizer and the model from pretrained tokenizers models
    """
    tokenizer = Wav2Vec2Tokenizer.from_pretrained("facebook/wav2vec2-base-960h")
    model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")    
    return tokenizer, model

def transcribe_and_translate(audio_folder, chunk_duration, mt_model_name):
  
    """
    Transcribes and translates audio files in a given folder using ASR and MT models.
    """
    #Set the length of each chunk
    max_chunk_length = 512 - 2 # Account for special tokens

    #Get list of all files in folder
    all_files = os.listdir(audio_folder)

    #Filter out only audio files
    audio_files = [filename for filename in all_files if filename.endswith((".wav", ".flac", ".mp3"))]

    #Initialize the speech-to-text model
    tokenizer, model = load_wav2vec_960h_model()

    #Initialize the machine translation model
    mt_tokenizer = FSMTTokenizer.from_pretrained(mt_model_name)
    mt_model = FSMTForConditionalGeneration.from_pretrained(mt_model_name)

    try:
        #Process each audio file
        for audio_file in audio_files:

            # Get CPU usage and memory consumption before loading audio file
            cpu_percent_start = psutil.cpu_percent()
            memory_mb_start = psutil.virtual_memory().used / 1e6

            #Load the audio file
            audio, sample_rate = sf.read(os.path.join(audio_folder, audio_file))

            # Check audio duration
            audio_duration = len(audio) / sample_rate
            if audio_duration <= 40:
                # Process audio file directly if duration is 40 seconds or shorter
                #Resample to 16kHz if necessary
                if sample_rate != 16000:
                    audio = librosa.resample(audio, sample_rate, 16000)
                    sample_rate = 16000

                #Tokenize the audio
                inputs = tokenizer(audio, return_tensors="pt", padding=True)

                #Generate the transcript
                start_time = time.time()
                with torch.no_grad():
                    logits = model(inputs.input_values).logits
                predicted_ids = torch.argmax(logits, dim=-1)
                transcription_chunk = tokenizer.decode(predicted_ids[0])
                transcription = transcription_chunk.lower()
                transcription_time = time.time() - start_time

                #Generate translation
                start_time = time.time()
                input_ids = mt_tokenizer.encode(transcription, return_tensors="pt")
                output = mt_model.generate(input_ids, max_new_tokens=4000, num_beams=18)
                decoded_text = mt_tokenizer.decode(output[0], skip_special_tokens=True)
                translation_time = time.time() - start_time

                # Get CPU usage and memory consumption
                cpu_percent = psutil.cpu_percent()
                memory_mb = psutil.virtual_memory().used / 1e6

                #Print the transcription and translation
                print(f"Transcription of {audio_file}: {transcription}")
                print(f"Translation of {audio_file}: {decoded_text}")
                print(f"CPU usage for audio file {audio_file}: {cpu_percent:.2f}%")
                print(f"Memory consumption for audio file {audio_file}: {memory_mb:.2f} MB")
                print(f"Transcription Time: {transcription_time:.2f} seconds")
                print(f"Translation Time: {translation_time:.2f} seconds")
                print(f"Total Time: {transcription_time+translation_time:.2f} seconds")

            else:
                # Split the audio into chunks
                num_chunks = int(len(audio) / sample_rate / chunk_duration) + 1
                chunks = [audio[i * sample_rate * chunk_duration: (i+1) * sample_rate * chunk_duration] for i in range(num_chunks)]

                # Initialize the transcription and translation timers
                transcribe_start_time = time.time()
                translate_start_time = time.time()

                #Process each chunk of audio
                transcription = ""
                for chunk in chunks:
                    #Resample to 16kHz if necessary
                    if sample_rate != 16000:
                        chunk = librosa.resample(chunk, sample_rate, 16000)

                    #Tokenize the audio
                    inputs = tokenizer(chunk, return_tensors="pt", padding=True)

                    #Generate the transcript
                    with torch.no_grad():
                        logits = model(inputs.input_values).logits
                    predicted_ids = torch.argmax(logits, dim=-1)
                    transcription_chunk = tokenizer.decode(predicted_ids[0])
                    transcription += transcription_chunk + " "

                #Lowercase the transcription
                transcription = transcription.lower()

                #Split input text into smaller chunks
                input_chunks = []
                for i in range(0, len(transcription), max_chunk_length):
                    input_chunk = transcription[i:i + max_chunk_length]
                    input_chunks.append(input_chunk)

                #Generate outputs for each chunk
                outputs = []
                for input_chunk in input_chunks:
                    input_ids = mt_tokenizer.encode(input_chunk, return_tensors="pt")

                    # Calculate time elapsed for each translation
                    elapsed_time = time.time() - translate_start_time
                    print(f"Elapsed time for translation of {audio_file}: {elapsed_time:.2f} seconds")

                    output = mt_model.generate(input_ids, max_new_tokens=4000, num_beams=18)
                    outputs.append(output)

                #Concatenate output for each chunk and decode
                decoded_outputs = [mt_tokenizer.decode(output[0], skip_special_tokens=True) for output in outputs]
                decoded_text = "".join(decoded_outputs)

                # Calculate time elapsed for transcription + translation
                elapsed_time = time.time() - transcribe_start_time
                print(f"Elapsed time for transcription + translation of {audio_file}: {elapsed_time:.2f} seconds")

                #Print the transcription and translation
                print(f"Transcription of {audio_file}: {transcription}")
                print(f"Translation of {audio_file}: {decoded_text}")

    except Exception as e:
        #Print an error message if the audio file or translation cannot be opened
        print(f"Error: {e}")
audio_folder = '/content/drive/My Drive/Ted'
chunk_duration = 30 #In seconds
mt_model_name = "facebook/wmt19-en-ru" 

transcribe_and_translate(audio_folder, chunk_duration, mt_model_name)
